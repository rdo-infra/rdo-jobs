{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "ad08ab2f_3895a419",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 602
      },
      "writtenOn": "2024-10-19T11:41:34Z",
      "side": 1,
      "message": "Good (partial) results from the testprojects:\n\n```\n[ceph: root@np0004955600 /]# ceph versions\n{\n    \"mon\": {\n        \"ceph version 18.2.1-229.el9cp (ef652b206f2487adfc86613646a4cac946f6b4e0) reef (stable)\": 3\n    },\n    \"mgr\": {\n        \"ceph version 18.2.1-229.el9cp (ef652b206f2487adfc86613646a4cac946f6b4e0) reef (stable)\": 3\n    },\n    \"osd\": {\n        \"ceph version 18.2.1-229.el9cp (ef652b206f2487adfc86613646a4cac946f6b4e0) reef (stable)\": 3\n    },\n    \"mds\": {\n        \"ceph version 18.2.1-229.el9cp (ef652b206f2487adfc86613646a4cac946f6b4e0) reef (stable)\": 3\n    },\n    \"overall\": {\n        \"ceph version 18.2.1-229.el9cp (ef652b206f2487adfc86613646a4cac946f6b4e0) reef (stable)\": 12\n    }\n}\n```\n\nand the cluster is healthy:\n\n\n```\n[ceph: root@np0004955600 /]# ceph -s\n  cluster:\n    id:     742860c4-2bdf-5098-b14f-ce122d7a9d5a\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum np0004955600,np0004955628,np0004955627 (age 41m)\n    mgr: np0004955600.pykami(active, since 43m), standbys: np0004955628.xsixgu, np0004955627.saudib\n    mds: 1/1 daemons up, 2 standby\n    osd: 3 osds: 3 up (since 40m), 3 in (since 40m)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   7 pools, 177 pgs\n    objects: 24 objects, 451 KiB\n    usage:   135 MiB used, 21 GiB / 21 GiB avail\n    pgs:     177 active+clean\n````",
      "revId": "058e6b280c60aac7be6c37fb5e5b5eb4290cc2c3",
      "serverId": "270e2033-b340-4cff-9539-693957ebf0e7"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "fa8a956d_1c87a905",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 192
      },
      "writtenOn": "2024-10-21T06:58:51Z",
      "side": 1,
      "message": "commented on the depends-on this morning and I think it is ready to merge with a green test run (please see sanity check there about ceph grafana image)",
      "revId": "058e6b280c60aac7be6c37fb5e5b5eb4290cc2c3",
      "serverId": "270e2033-b340-4cff-9539-693957ebf0e7"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1b13ba22_720e31f0",
        "filename": "playbooks/data_plane_adoption/deploy_tripleo_run_repo_tests.yaml",
        "patchSetId": 4
      },
      "lineNbr": 52,
      "author": {
        "id": 192
      },
      "writtenOn": "2024-10-21T06:58:51Z",
      "side": 1,
      "message": "not related to the bump just noticed we could be using the osp_17_ceph_repos var here",
      "revId": "058e6b280c60aac7be6c37fb5e5b5eb4290cc2c3",
      "serverId": "270e2033-b340-4cff-9539-693957ebf0e7"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8224b731_1eff2cd4",
        "filename": "playbooks/data_plane_adoption/deploy_tripleo_run_repo_tests.yaml",
        "patchSetId": 4
      },
      "lineNbr": 52,
      "author": {
        "id": 602
      },
      "writtenOn": "2024-10-21T07:13:54Z",
      "side": 1,
      "message": "I\u0027ll follow up on this after landing the main patch. Note I\u0027m simply trying to reduce the scope of the changes to move to RHCS 7. But I\u0027m ok to optimize more the code involved there.",
      "parentUuid": "1b13ba22_720e31f0",
      "revId": "058e6b280c60aac7be6c37fb5e5b5eb4290cc2c3",
      "serverId": "270e2033-b340-4cff-9539-693957ebf0e7"
    }
  ]
}