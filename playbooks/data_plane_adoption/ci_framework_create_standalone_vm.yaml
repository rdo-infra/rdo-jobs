- hosts: all
  vars:
    framework_dir: "/home/zuul/src/github.com/openstack-k8s-operators/ci-framework"
  tasks:
    - name: Configure qemu and vm network for extracted crc
      when: "'extracted-crc' in job_name"
      block:
        - name: Enable ip forwarding
          become: true
          ansible.builtin.shell:
            sysctl -w net.ipv4.ip_forward=1

        - name: Get crc node MAC address for later use
          # register it now from the eth1 interface, instead of later
          # with the br1 brideg, as that could cause timing
          # issues depending on how quickly the bridge comes up
          ansible.builtin.shell:
            ip neigh get 192.168.122.10 dev eth1 | cut -d ' ' -f 5
          register: crc_mac

        - name: Create bridge to connect standalone vm to 192.168.122 network
          become: true
          community.general.nmcli:
            type: bridge
            conn_name: br1
            ifname: br1
            gw4: 192.168.122.1
            dns4:
              - 192.168.122.10
            ip4:
              - 192.168.122.11/24
            state: present

        - name: WA add static ARP entries to acces the k8s LB
          become: true
          ansible.builtin.shell: >
            dnf install net-tools -y
            arp -s 172.17.0.85 {{ crc_mac.stdout }}
            arp -s 172.17.0.86 {{ crc_mac.stdout }}

        - name: Turn down ci-private-network connection
          become: true
          community.general.nmcli:
            conn_name: ci-private-network
            state: absent

        - name: Set ci-private-network connection as slave of the br1 bridge
          become: true
          community.general.nmcli:
            slave_type: bridge
            type: ethernet
            ifname: eth1
            master: br1
            conn_name: ci-private-network
            state: present

        - name: Turn down vlan interfaces to allow tagged traffic to be forwarded to vm
          # jgilaber: keep vlan20 interface since we need it to ping ovn pods over the
          # internal-api network, if in the future we move the ovn adoption to
          # execute with 'oc run' it should be added here
          become: true
          community.general.nmcli:
            conn_name: "ci-private-network-{{ item }}"
            state: absent
          loop:
            - 21
            - 22

        - name: Ensure qemu is configured to use the zuul user
          become: true
          ansible.builtin.lineinfile:
            path: "/etc/libvirt/qemu.conf"
            regexp: "{{ item.regexp }}"
            line: "{{ item.line }}"
            backrefs: true
          loop:
            - {regexp: '^#user =', line: 'user = "zuul"'}
            - {regexp: '^#group =', line: 'group = "libvirt"'}

        - name: Restart libvirtd service to apply the changes
          become: true
          ansible.builtin.service:
            name: libvirtd
            state: restarted

        - name: Make sure virtqemud service is running
          become: true
          ansible.builtin.service:
            name: virtqemud
            state: started

        - name: Make sure virtnetworkd service is running
          become: true
          ansible.builtin.service:
            name: virtnetworkd
            state: started

        - name: Undefine existing default network
          community.libvirt.virt_net:
            name: default
            state: absent

        - name: Recreate libvirt default with the correct network ip range
          community.libvirt.virt_net:
            name: default
            state: present
            xml: '{{ lookup("file", "libvirt_default_network.xml") }}'

        - name: Activate default network
          community.libvirt.virt_net:
            name: default
            state: active

        - name: Set default network to autostart
          community.libvirt.virt_net:
            name: default
            autostart: true

        - name: Catch packets to multicast adresses
          become: true
          ansible.builtin.iptables:
            table: "nat"
            chain: "LIBVIRT_PRT"
            source: "192.168.122.0/24"
            destination: "224.0.0.0/24"
            jump: "RETURN"

        - name: Catch packets to broadcast adresses
          become: true
          ansible.builtin.iptables:
            table: "nat"
            chain: "LIBVIRT_PRT"
            source: "192.168.122.0/24"
            destination: "255.255.255.255/32"
            jump: "RETURN"

        - name: Masquerade outgoing tcp packets
          become: true
          ansible.builtin.iptables:
            table: "nat"
            chain: "LIBVIRT_PRT"
            source: "192.168.122.0/24"
            destination: "!192.168.122.0/24"
            jump: "MASQUERADE"
            protocol: tcp
            to_ports: "1024-65535"

        - name: Masquerade outgoing udp packets
          become: true
          ansible.builtin.iptables:
            table: "nat"
            chain: "LIBVIRT_PRT"
            source: "192.168.122.0/24"
            destination: "!192.168.122.0/24"
            jump: "MASQUERADE"
            protocol: udp
            to_ports: "1024-65535"

        - name: Masquerade outgoing packets
          become: true
          ansible.builtin.iptables:
            table: "nat"
            chain: "LIBVIRT_PRT"
            source: "192.168.122.0/24"
            destination: "!192.168.122.0/24"
            jump: "MASQUERADE"

        - name: Add ebtables rule to make sure that vm gets arp replies from crc
          # FIXME(jgilaber): This is a workaround needed to make the connection
          # between the standalone vm and the crc node stable, otherwise for some
          # reason the arp reply from the crc node is not always forwarded to the vm
          become: true
          ansible.builtin.shell: >
            ebtables -t nat -A PREROUTING --logical-in br1 -p arp
            --arp-ip-dst 192.168.122.10 --arp-opcode Request -j arpreply
            --arpreply-mac "{{ crc_mac.stdout }}"

    - name: Create a vm and deploy standalone on it with network isolation
      ansible.builtin.include_role:
        name: 'install_yamls_makes'
        tasks_from: 'make_standalone.yml'
      vars:
        make_standalone_params:
          EDPM_COMPUTE_VCPUS: 8
          EDPM_COMPUTE_RAM: 7
          EDPM_COMPUTE_DISK_SIZE: 70
          NTP_SERVER: "{{ ntp_override | default('pool.ntp.org') }}"
          REPO_SETUP_CMDS: "{{ repo_setup_commands | default('/tmp/standalone_repos') }}"
          GATEWAY: "{{ standalone_gateway }}"
          IP: "{{ standalone_ip }}"
          DATAPLANE_DNS_SERVER: "{{ standalone_dns | default(standalone_gateway) }}"
          HOST_PRIMARY_RESOLV_CONF_ENTRY: "{{ standalone_dns | default(standalone_gateway) }}"

    - name: Create a snapshot of the standalone vm in case we want to retry the adoption process
      ansible.builtin.include_role:
        name: 'install_yamls_makes'
        tasks_from: 'make_standalone_snapshot.yml'
